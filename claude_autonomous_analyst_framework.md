# Claude Autonomous Analyst Framework
## Multi-Source Development Pattern Discovery

### üéØ **Mission Statement**
Claude acts as an autonomous data analyst, systematically exploring PostgreSQL/pgvector, Neo4j, and Elasticsearch to discover development patterns, generate new hypotheses, and create actionable insights through iterative investigation.

### üìä **Data Source Capabilities**

**PostgreSQL + pgvector:**
- Semantic similarity and content clustering
- Vector embeddings for concept relationships  
- Traditional relational analysis
- Time-series content evolution

**Neo4j:**
- Temporal workflow sequences
- Causal relationship mapping
- Problem ‚Üí Solution chains
- Collaborative patterns and knowledge transfer

**Elasticsearch:**
- Statistical aggregations and multi-dimensional analysis
- Productivity pattern mining
- Domain expertise evolution
- Faceted search and correlation discovery

---

## üîç **Phase 1: Foundation Discovery Prompts**

### **Cross-Source Pattern Validation**
```
PROMPT_001: "Compare the top 5 productivity patterns discovered in Elasticsearch hourly aggregations with Neo4j temporal sequences. Do they tell the same story? Where do they diverge?"

PROMPT_002: "Use pgvector to find semantically similar memory clusters, then trace those same memory IDs through Neo4j workflows. What causal chains emerge from similar content?"

PROMPT_003: "Elasticsearch shows domain expertise evolution - validate this against Neo4j relationship depth over time. Does relationship complexity increase with domain experience?"
```

### **Unique Capability Exploration**
```
PROMPT_004: "What can Neo4j reveal about workflow bottlenecks that Elasticsearch aggregations miss? Focus on sequence interruptions and backtracking patterns."

PROMPT_005: "Use pgvector to discover conceptual themes that don't appear in explicit tags. What hidden patterns emerge from semantic clustering?"

PROMPT_006: "Elasticsearch can aggregate by sentiment + complexity + time simultaneously. What 3D patterns emerge that single-dimension analysis misses?"
```

---

## üß† **Phase 2: Hypothesis Generation Prompts**

### **Problem-Solution Intelligence**
```
PROMPT_007: "Neo4j: Map all problem ‚Üí solution chains. Which solution types have the highest success rates for specific problem patterns?"

PROMPT_008: "Elasticsearch: Correlate problem occurrence times with productivity metrics. Do problems cluster during low-productivity windows or cause them?"

PROMPT_009: "pgvector: Find memories semantically similar to successful problem resolutions. What common language patterns predict solution success?"
```

### **Learning Curve Analysis**
```
PROMPT_010: "Neo4j: Trace expertise evolution paths - how do relationship patterns change as someone masters a domain?"

PROMPT_011: "Elasticsearch: Analyze complexity progression over time within technical domains. Does complexity increase linearly or in jumps?"

PROMPT_012: "pgvector: Compare semantic content richness between early and late memories in the same domain. How does vocabulary evolve?"
```

### **Collaboration Patterns**
```
PROMPT_013: "Neo4j: Identify collaborative memory chains - which workflows benefit most from multi-person involvement?"

PROMPT_014: "Elasticsearch: Compare individual vs collaborative work sentiment and complexity distributions. When is collaboration most effective?"

PROMPT_015: "pgvector: Find semantic similarity between memories tagged with collaboration indicators. What collaborative concepts cluster together?"
```

---

## üî¨ **Phase 3: Deep Investigation Prompts**

### **Workflow Optimization**
```
PROMPT_016: "Cross-reference Neo4j sequence analysis with Elasticsearch productivity hotspots. Design an optimal daily workflow template."

PROMPT_017: "Use pgvector similarity to group planning memories, then trace their Neo4j implementation sequences. What planning patterns lead to smoother execution?"

PROMPT_018: "Elasticsearch: Find complexity escalation trigger points, then use Neo4j to trace what workflow decisions led to those escalations."
```

### **Predictive Pattern Discovery**
```
PROMPT_019: "Neo4j: Identify early warning signals in workflow sequences that predict future problems. Can we create a 'risk score' for development paths?"

PROMPT_020: "Elasticsearch: Build statistical models for productivity prediction based on historical patterns. What variables have the strongest correlation?"

PROMPT_021: "pgvector: Find semantic patterns that precede breakthrough moments or major insights. What language signals indicate upcoming discoveries?"
```

### **Domain-Specific Intelligence**
```
PROMPT_022: "For each technical domain (testing, frontend, backend, etc.), compare the three data sources. Which domains show the most/least consistent patterns across sources?"

PROMPT_023: "Neo4j: Map knowledge transfer patterns between domains. How do insights from one area influence work in another?"

PROMPT_024: "Elasticsearch: Create domain expertise maturation profiles. What does mastery look like statistically in each technical area?"
```

---

## üß™ **Phase 4: Meta-Analysis & Discovery Generation**

### **Claude's Autonomous Investigation**
```
PROMPT_025: "Based on patterns found so far, generate 5 new hypotheses about development workflow optimization. Design specific queries for each data source to test these hypotheses."

PROMPT_026: "Identify the biggest contradiction between what the three data sources reveal. Investigate this contradiction and propose explanations."

PROMPT_027: "What question would provide the most actionable insight if answered? Design a multi-source investigation to answer it."
```

### **Pattern Synthesis**
```
PROMPT_028: "Create a unified model of effective development workflow that incorporates insights from all three data sources. Where do they complement vs contradict each other?"

PROMPT_029: "Design early warning systems for common development problems using triggers from all three sources. What would a 'development health dashboard' look like?"

PROMPT_030: "Synthesize domain expertise development pathways. How should someone systematically build expertise in a new technical area?"
```

---

## ü§ñ **Claude Autonomous Analysis Protocol**

### **Investigation Methodology**
1. **Start with Foundation Prompts** - Establish baseline understanding
2. **Generate Hypotheses** - Based on initial findings, create new investigation questions  
3. **Deep Dive** - Use generated hypotheses to drive focused analysis
4. **Meta-Analysis** - Step back and synthesize findings
5. **New Prompt Generation** - Create follow-up prompts based on discoveries
6. **Memory Creation** - Store new insights and prompt discoveries
7. **Report Generation** - Comprehensive findings summary

### **Memory Creation Protocol**
For each significant discovery, Claude should store:
- **Discovery Type**: Pattern/Hypothesis/Contradiction/Insight
- **Data Sources Used**: Which combination provided the insight
- **Evidence Strength**: High/Medium/Low confidence
- **Actionable Implications**: Specific workflow recommendations
- **Follow-up Questions**: New prompts generated from this discovery

### **Report Structure**
```
# Development Pattern Analysis Report
## Executive Summary
## Data Source Comparison
## Key Discoveries by Category
## Cross-Source Pattern Validation
## Generated Hypotheses & Tests
## Workflow Optimization Recommendations
## Early Warning System Design
## Domain-Specific Insights
## Contradictions & Unresolved Questions
## Next Investigation Priorities
```

---

## üéõÔ∏è **Implementation Framework**

### **Claude's Investigation Toolkit**
- Access to all three data sources with read/query capabilities
- Memory creation tools for storing discoveries
- Report generation and update capabilities
- Hypothesis tracking and validation workflow
- Cross-reference and correlation analysis tools

### **Success Metrics**
- Number of actionable insights discovered
- Cross-source pattern validation rate
- New hypothesis generation quality
- Workflow optimization impact potential
- Discovery-to-memory conversion effectiveness

This framework turns Claude into an autonomous research assistant that can systematically explore complex multi-dimensional data, generate its own investigation paths, and create valuable insights that no single data source could provide alone.